Problem Set 1

DueDate (2/25/2014)


Sangsoo Park
========================================================

1. Derive the SLR estimator $\hat{\beta_0}$ and $\hat{\beta_1}$
($\mu(x)$ = mean of x, $\mu(y)$ = mean of y)

$$Q = \sum_{i=1}^{n}(y_i - \hat{\beta_0} + \hat{\beta_1}*x_i))^2$$
$$= \sum_{i=1}^{n}(y_i^2 - 2*y_i*\hat{\beta_0} + \hat{\beta_1}*x_i) + (\hat{\beta_0} + \hat{\beta_1}*x_i)^2)$$
  
  To minimize Q, partial derivative of Q by either $\hat{\beta_0}$ or $\hat{\beta_1}$ must be zero

  1) With regard to $\hat{\beta_0}$
  
  $$\frac{\partial Q}{\partial \hat{\beta_0}} = \sum_{i=1}^{n}(-2*y_i + 2*\hat{\beta_0} + 2*\hat{\beta_1}*x_i)$$
  $$= \sum_{i=1}^{n}(y_i) - \hat{\beta_0}*n - \hat{\beta_1} * \sum_{i=1}^{n}(x_i) = 0$$
  $$\hat{\beta_0} = \frac{\sum_{i=1}^{n}(y_i)}{n} - \hat{\beta_1} * \frac{\sum_{i=1}^{n}(x_i)}{n}$$
  $$\hat{\beta_0}= \mu(y)- \hat{\beta_1}* \mu(x)$$
  
  2) With regard to $\hat{\beta_1}$
  
  $$\frac{\partial Q}{\partial \hat{\beta_1}} = \sum_{i=1}^{n}(-2*x_i*y_i + 2*\hat{\beta_0}*x_i + 2*\hat{\beta_1}*x_i^2) = 0$$
  $$\sum_{i=1}^{n}(\hat{\beta_1}*x_i^2) = \sum_{i=1}^{n}(x_i*y_i) - \sum_{i=1}^{n}(\hat{\beta_0}*x_i)$$ 
  $$= \sum_{i=1}^{n}(x_i*y_i) - (\mu(y)-\hat{\beta_1}*\mu(x))*\sum_{i=1}^{n}(x_i)$$ 
  $$= \sum_{i=1}^{n}(x_i*y_i) - \mu(y) * \sum_{i=1}^{n}(x_i) + \hat{\beta_1}*\mu(x) * \sum_{i=1}^{n}(x_i)$$
  $$\hat{\beta_1}*(\sum_{i=1}^{n}(x_i^2) - \mu(x) * \sum_{i=1}^{n}(x_i))$$
  $$= \sum_{i=1}^{n}(x_i*(y_i - \mu(y)))$$
  $$\hat{\beta_1} = \frac{\sum_{i=1}^{n}(x_i*(y_i-\mu(y)))}{\sum_{i=1}^{n}(x_i*(x_i-\mu(x)))}$$


1.5
  - I don't agree with it because one of the assumptions was that summation of error at a given X value should be zero in linear regression model. Thus, we can remove the error term. (expected Y => fitted values by linear regression model)


1.6
  (a)   
    Red lines are upper and under bound of 1 standard deviation from the regression line
  
```{r}
library(ggplot2)
B_0 <- 200
B_1 <- 5
stan <- 4
# 
X <- seq(from=0,to=50,by=10)
Y <- B_0 + B_1 * X
data_p <- data.frame(X,Y,4)
names(data_p) <- c("X","Y","STD")
p4 <- ggplot(data_p,aes(x=X, y=Y),colour="red") + geom_ribbon(aes(ymin=Y-STD,
               ymax=Y+STD), alpha=.05, colour="red") + theme_bw()
p4
```
  
  (b) Parameter $\beta_0$ - expected Y value when X value is zero (X=0)
  
  Parameter $\beta_1$ : the amount of change in Y value when a unit of X value increases

1.7
  (a) $\beta_0$ = 100, $\beta_1$ = 20 => Y = 100 + 20 * X
  
      Expected Y value is 200 when the X value is 5,
      variance of the expected Y value is 25
      => We don't know what distribution the error term has because it is unspecified.
      
  (b) 
    If the normal error regression model is applicable, expected Y also follow the normal distribution.This suggests that we can use the density function for a normal probability distribution to calculate the probability using the mean and standard deviation
  
1.10
    - The curvilinear relationshp indicates that changes in Salary at changes in relatively young age is larger than ones at changes in relativley old age. The slopes of the curvilinear relationship are postivie regardless of X values (here, ages). That is, the salary increases as the age increases until ages reaches 47 years.However, we don't know what happens after the age. 
  
1.11 
    - There is a linear relationship between the two variables, outputs from pre  - and post-training. The training program becomes less effective as X variable vales are increased while the program is effective for all X variable values because it has a postivie slope (0.95). 
  
1.27
    (a) The linear regression function below fit the data points well.We can see symmetrical distribution of residuals and constant variability of the residuals arcoss x values (See Normal Q-Q Plot below). The negative correlation indicated that muscle mass decreases with age.
  
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR27.txt", destfile="CH01PR27.txt")
pr27 = read.table("CH01PR27.txt", col.names=c("age", "mass"))
m <- lm(mass~age, data=pr27)
summary(m)
# Check linearilty of model
plot(pr27$age,pr27$mass,xlab="Age",ylab="Muscle Mass")
abline(m)

# Verity the linearilty as well as constant variability
plot(m$residuals ~ pr27$age)
abline(h = 0, lty = 3)

# Check distribution of residuals
qqnorm(m$residuals)
qqline(m$residuals)
```

  (b)
    (1) residuals at change in age by one year

```{r}
residuals <- resid(m)
head(residuals)

```

    (2) at X=60, (3) at 8th case of the X, (4) 35.38271
```{r}
c <- coef(m)
# (2) at age 60yr
Y = c[1] + c[2]*60
print(Y)

# (3) 
Z <- fitted(m)
Z[8]

# (4) MSE = SSE / n-2
n <- length(Z)
sum(m$residuals^2)/(n-2)

```

1.28
    (a) Generally, the estimated regression function appeard as a good fit and showed a negative relationship between the level of education and crime rate in medium-sized U.S. countries. It seems like there are larger variability of erros across the x values compared to variability of the previous data set(1.27), but below Q-Q plot showed that the variability across the x values is constant. The residuals were also distributed evenly or symmetrically, which indicates mean of erros at a given x value is zero.
  
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR28.txt", destfile="CH01PR28.txt")
pr28 = read.table("CH01PR28.txt", col.names=c("Crime_rate","Percent"))
m1 <- lm(Crime_rate~Percent, data=pr28)
summary(m1)

# Check linearilty of model
plot(pr28$Percent,pr28$Crime_rate,xlab="% of individual with at least highschool diploma",ylab="Crime rate")
abline(m1)

# Verity the linearilty as well as constant variability
plot(m1$residuals ~ pr28$Percent)
abline(h = 0, lty = 3)

# Check distribution of residuals
qqnorm(m1$residuals)
qqline(m1$residuals)

```

  (b)
    (1)

```{r}
residuals <- resid(m1)
head(residuals)

```

    (2) at X=80, (3) at 10th case of the X, (4) 5,552,112
```{r}
c1 <- coef(m1)
# (2) at age 60yr
Y2 = c1[1] + c1[2]*60
print(Y2)

# (3) use fitted function
m1$residuals[10]

# (4)
n <- length(m1$residuals)
sum(m1$residuals^2)/(n-2)

```

1.36 sum(Yhat*error)
     => sum of error == zero (1.17)
     => sum of X*error = zero (1.19)
     => Thus, sum of Yhat*error also have to be zero

1.42
    a. When B1 = 17, 18, 19
    b. B1 = 18, the likelihood function is largest
    c. (B) is consistent with (c) B1=17.93 if we can round the B1 value from (c)
    d. I am not sure how to do use a computer graphics or statistics package for the likelihood function. Could you give me feedback on this?
  
```{r}   
  B1 <- as.matrix(c(17,18,19))
  x_142 <- as.matrix(c(7,12,4,14,25,30))
  u_142 <- as.matrix(c(128,213,75,250,446,540))
  y_142 <- matrix(0,6,1) 
  
  k <- 0 
  max_likely <- matrix(0,3,1)
  f <- matrix(0,6,1)

for (j in 1:3) {
  
  y_142 <- B1[j,1] * x_142 
  
  for (i in 1:nrow(x_142)){

    # State of likelihood function at a given X
    k <- ((u_142[i,1] - y_142[i,1])/4)^2
    f[i,1] <- (1/(sqrt(2*3.14)*4))*exp((-1/2)*k)
    
  }
  
  max_likely[j,1] <- f[1,1]*f[2,1]*f[3,1]*f[4,1]*f[5,1]*f[6,1]
    
}

#(b)When B1 = 18, the likelihood function is largest(second row value came out)
max_likely
max(max_likely)

# (c) When B1 = 17.93, the likelihood function is largest
b1_estimator <- sum(x_142*u_142)/sum(x_142^2)
b1_estimator
```

Chapter 6

6.2
    a. Matrix expression when i = 1~5 and When we assume x_i_1^2 = x_i_3
    
```{r}
# Y = Xb + e
Y <- as.matrix(c("Y1","Y2","Y3","Y4","Y5"))
X <- matrix(c(1,1,1,1,1,"X11","X21","X31","X41","X51","x12","x22","x32","x42",
         "x52","xx13","xx23","xx33","xx43","xx53"), nrow=5)
b <- matrix(c("0","b1","b2","b3"))
e <- matrix(c("e1","e2","e3","e4","e5"))
Y
X
b
e
```

  b. $\sqrt{Y_i} = \hat{Y_i}$ and $\log_{10}X_2 = \hat{X_2}$
```{r}
# Y = Xb + e
Y_hat <- as.matrix(c("Y1","Y2","Y3","Y4","Y5"))
X <- matrix(c(1,1,1,1,1,"X11","X21","X31","X41","X51","x12","x22","x32","x42",
         "x52"), nrow=5)
b <- matrix(c("1","b1","b2"))
e <- matrix(c("e1","e2","e3","e4","e5"))
Y_hat
X
b
e
```
  
6.3

    Adding more predictor variables could cause increase in squared R value because at least small variance of data can be explained by the variables. owever, we don't know whether adding those variables leads to significant increase in squared R value or not. 
    
6.5
  a. scatter plot matrix and correlation matrix : Those could allow us to know information on relationship between the predictor variables and the response variables in advance to running linear regression analysis.That is, we could look at linearity between two variables (See below). 
  
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%206%20Data%20Sets/CH06PR05.txt", destfile="CH06PR05.txt")
pr05 = read.table("CH06PR05.txt", col.names=c("y","x1", "x2"))
# Scatter plot
plot(pr05[1:3])
pairs(pr05)

# Correlation matrix
as.matrix(cor(pr05))
```

    b. Fit regression model (6,1) : b1 indicates the amount of changes in the response variable at a one unit increase of X1 (moisture content) while X2 (sweetness) isn't changed. 
    
```{r}
multi_pr05 <- lm(formula = y ~ x1 + x2, data = pr05)
summary(multi_pr05)
```
    c. Residuals and a box plot of the residuals. The box plot showed 25 and 75 percentiles as well as median. Thus, we can see distribution of the residuals (can check whether the residuals are from the normal distribution). Based on below box plot, we can see tha the residuals from the lienar regression model are nomally distributed. 

```{r}
# Box plot
boxplot(multi_pr05$residuals)
```

  d. 
    - residuals against : We can see that there is no trend of the residuals and no outliers of the residuals below plots. Thus, the residuals have constant variance and the linear regression model satisfied one of the asummptions.
  
```{r}
# Residuals against y(hat)
plot(multi_pr05$fitted, multi_pr05$residuals)

# Residuals against x1
plot(pr05$x1, multi_pr05$residuals)

# Residuals against x2
plot(pr05$x2, multi_pr05$residuals)

# Residuals against x1*x2
plot(pr05$x1 * pr05$x2, multi_pr05$residuals)
```

    - Normal probability plot : We can see almost perfect linear line in the normal Q-Q plot. It suggests that the residuals follow the normal distribution.
```{r}
qqnorm(multi_pr05$residuals)
qqline(multi_pr05$residuals)
```

6.9    
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%206%20Data%20Sets/CH06PR09.txt", destfile="CH06PR09.txt")
pr09 = read.table("CH06PR09.txt", col.names=c("y","x1", "x2","x3"))
```
  a. 
    - There were gaps from 4600 to 4800 in the Y (x-axis) when X1, X2, and X3 variables are located at y axis (lower triangle of the scatter plot matrix)
```{r}
plot(pr09)
```
    - There are no outliers based on below stem-and leaf plots. 
```{r}
stem(pr09$x1)
stem(pr09$x2)
```

    b.Below plots showed how each variable changes as a function of time (Unit: week). The variable y and x2 showed fluctuation across the times while the variable x1 showed higher numbers at the end of the year. 
```{r}
x <- c(1:nrow(pr09))
plot(x,pr09$y, type='l',xlab="Time(week)", ylab="Total Labor hours")
plot(x,pr09$x1, type='l',xlab="Time(week)", ylab="# of cases shipped")
plot(x,pr09$x2, type='l',xlab="Time(week)", ylab="Indirect cost(%)")
plot(x,pr09$x3, xlab="Time(week)", ylab="Week(no holiday=0)")
```

    c.We can check correlation values and relationship between all possible combinations of two variables. The total labor hours were larger when a week has holiday (r=0.81) while the other r values were very low. 
```{r}
# Scatter plot
pairs(pr09)

# Correlation matrix
as.matrix(cor(pr09))
```

6.10
    a. fit regression model
      y = 415 - 0.000787*x1 - 13.17*x2 + 623.6*x3 
      Here, b1 indicated that a one unit increase in x1 decreases -0.00079 of y value while the other variables are fixed or were not changed. We can interpret b2 and b3 each for x2 and x3 by the same way.
```{r}
multi_pr09 <- lm(formula = y ~ x1 + x2 + x3, data = pr09)
summary(multi_pr09)
```
    b. a box plot : The boxplot looks like a normal distribution with almost zero mean and symmatrical shape. Thus, the regression results showed that the regression model didn't violate two assumptions: 1) summation of error is zero and 2) normal distribution. 
```{r}
boxplot(multi_pr09$residuals)
```

  d. The errors was not correlated each other.
```{r}
x <- c(1:nrow(pr09))
plot(x, multi_pr09$residuals)
```

6.15
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%206%20Data%20Sets/CH06PR15.txt", destfile="CH06PR15.txt")
pr15 = read.table("CH06PR15.txt", col.names=c("y","x1", "x2","x3"))
```
  a.  
    a stem and leaf plot for each of the predictor variables. For the variable X1, the stem plot looks like a biomdal distribution. However, the other two variables looks like the normal distribution. 
```{r}
stem(pr15$x1)
stem(pr15$x2)
stem(pr15$x3)
```

    b. 
    Each of x1, x2, and x3 variable showed negative correlation with y variable. We can clearly look at the relationship from below scatter plot and correlation matrix. 
```{r}
# Scatter plot
pairs(pr15)

# Correlation matrix
as.matrix(cor(pr15))
```

    c.b2 indicated that a one unit increase in x2 decreases 0.442 of y value while the other variables are fixed or were not changed.
```{r}
multi_pr15 <- lm(formula = y ~ x1 + x2 + x3, data = pr15)
summary(multi_pr15)
```

    d. There were no outliers in below box plot.
```{r}
# Obtain the residuals (only for the first part visualized)
head(multi_pr15$residuals)
# Box plot of the residuals
boxplot(multi_pr15$residuals)
```

6.22
  a. It is a general linear regression model
  b. It is not because the coefficient is not linear and it cant not be expressed in the form of (6.17)
  c. It is not because the coefficient is not linear
  d. It is not because the coefficient is not linear and it cant not be expressed in the form of (6.17)
  e. It is not because the coefficient is not linear and it cant not be expressed in the form of (6.17)
  
6.31
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Appendix%20C%20Data%20Sets/APPENC01.txt", destfile="APPENC01.txt")
appenc01 = read.table("APPENC01.txt", col.names=c("ID","Lstay", "Age","Irisk",
                                              "RCratio","RXratio","Nbed",
                                              "Msaffil","Region","AveCensus",
                                              "Nnurse","FandS"))
```
```{r}
# X1 : age, X2 : routine culturing ratio, X3: averageCensus, X4 : FandS
# For each region

multi_r1 <- lm(formula = Irisk ~ Age+RCratio+AveCensus+FandS, data = subset(appenc01, appenc01$Region==1))
multi_r2 <- lm(formula = Irisk ~ Age+RCratio+AveCensus+FandS, data = subset(appenc01, appenc01$Region==2))
multi_r3 <- lm(formula = Irisk ~ Age+RCratio+AveCensus+FandS, data = subset(appenc01, appenc01$Region==3))
multi_r4 <- lm(formula = Irisk ~ Age+RCratio+AveCensus+FandS, data = subset(appenc01, appenc01$Region==4))

```
    a and b
    - Estimated regression functions for the four regions
    
    Region 1: y=-3.35 + 0.12*age + 0.0582*RCratio + 0.002*Census + 0.007*FandS
    Region 2: y= 2.29 + 0.005*age + 0.058*RCratio + 0.001*Census + 0.015*FandS
    Region 3: y=-0.14 + 0.03*age + 0.102*RCratio + 0.004*Census + 0.008*FandS
    Region 4: y= 1.57 + 0.04*age + 0.0588*RCratio - 0.0007*Census + 0.013*FandS
  
    The regression lines look similar each other except different intercept for region 3 and slope of Census for region 4. 

    c. For each region, see below. Based on the result, the MSE values across the regions were similar each other while the squared R values were different. Suared R value for the region 3 was larger than one from the other regions. Thus, the X variables explain more variance of the region 3. 

```{r}  
## Calculation of MSE and R^2 for each region
# Function to calculate MSE and R^2
mse_r <- function(data,n){
  
  region <- subset(data, data$Region==n)
  
  x_mat <- cbind(c(1),region$Age,region$RCratio,region$AveCensus,region$FandS)
# Calculation of coefficient
  b_mat <- solve((t(x_mat) %*% x_mat)) %*% t(x_mat) %*% region$Irisk

# Calculation of SSE
  sse<-t(region$Irisk)%*%region$Irisk - t(b_mat)%*% t(x_mat)%*%region$Irisk
  mse <- sse/(nrow(region)-5)

# Calculation of SSTO and R^2
  j <- matrix(1,nrow(region),nrow(region))
  ssto <- t(region$Irisk)%*%region$Irisk - (1/nrow(region)) * (t(region$Irisk)%*%j%*%region$Irisk)

  sqR <- 1 - (sse/ssto)
  
  return(cbind(mse, sqR))
}

region1 <- mse_r(appenc01,1)
region2 <- mse_r(appenc01,2)
region3 <- mse_r(appenc01,3)
region4 <- mse_r(appenc01,4)
result <- rbind(region1, region2, region3, region4)
colnames(result) <- c("MSE","R^2")
rownames(result) <- c("Region1","Region2","Region3","Region4")

# Show result
result
```  


    d. Box plot for residuals. In region 3, there were 3 outliers in the boxplot of the residuals while the other boxplots didn't have any outliers. In addition, the boxplot for the region 1 and 4 looks skewed. 
  
```{r}  
boxplot(multi_r1$residuals,main="Region 1")
boxplot(multi_r2$residuals,main="Region 2")
boxplot(multi_r3$residuals,main="Region 3")
boxplot(multi_r4$residuals,main="Region 4")
``` 
