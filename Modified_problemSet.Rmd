Modified_ProblemSet_DueDate (2/25/2014)
========================================================
1.5
  - I don't agree with it because one of the assumptions was that summation of error at a given X value should be zero. Thus, we can simplify more for the equation by removing the error term.
  
1.6
  (a) 
  
    Red lines are upper and under bound of 1 standard deviation from the regression line
  
```{r}
library(ggplot2)
B_0 <- 200
B_1 <- 5
stan <- 4
# 
X <- seq(from=0,to=50,by=10)
Y <- B_0 + B_1 * X
data_p <- data.frame(X,Y,4)
names(data_p) <- c("X","Y","STD")
p4 <- ggplot(data_p,aes(x=X, y=Y),colour="red") + geom_ribbon(aes(ymin=Y-STD,
               ymax=Y+STD), alpha=.05, colour="red") + theme_bw()
p4
```
  
  (b) 
      Parameter β0 : expected Y value when X value is zero (X=0)
  
      Parameter β1 : the amount of change in Y value when a unit of X value increases

1.7
  (a) β0 = 100, β1 = 20 => Y = 100 + 20 * X
  
      Expected Y value is 200 when the X value is 5,
      variance of the expected Y value is 25
      => We don't know what distribution the error term has because it is unspecified.
      
  (b) 
    The range from 195 to 205 indicates 1*standard deviation from the mean. This suggests that the probability will be around 68% if we assume that error has normal distribution.
  
1.10
    - The curvilinear relationshp indicates that changes in Salary at changes in relatively young age is larger than ones at changes in relativley old age. The slopes of the curvilinear relationship are postivie regardless of X values (here, ages). That is, the salary increases as the age increases until ages reaches 47 years.
  
1.11 
    - There is a linear relationship between the two variables, outputs from pre  - and post-training. We know that outputs after post-training is less than ones before pre-training. However, we can't know causation. There might be different reasons to lead to the result. 
  
1.27
    (a) The linear regression function below fit the data points well.We can see symmetrical distribution of residuals and constant variability of the residuals arcoss x values (See Normal Q-Q Plot below). The negative correlation indicated that muscle mass decreases with age.
  
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR27.txt", destfile="CH01PR27.txt")
pr27 = read.table("CH01PR27.txt", col.names=c("age", "mass"))
m <- lm(mass~age, data=pr27)
summary(m)
# Check linearilty of model
plot(pr27$age,pr27$mass,xlab="Age",ylab="Muscle Mass")
abline(m)

# Verity the linearilty as well as constant variability
plot(m$residuals ~ pr27$age)
abline(h = 0, lty = 3)

# Check distribution of residuals
qqnorm(m$residuals)
qqline(m$residuals)
```

  (b)
    (1) residuals at change in age by one year

```{r}
residuals <- resid(m)
head(residuals)

```

    (2) at X=60, (3) at 8th case of the X, (4) 2052.19
```{r}
c <- coef(m)
# (2) at age 60yr
Y = c[1] + c[2]*60
print(Y)

# (3) expected Y value at 8th case of the X
Y1 = c[1] + c[2]*pr27[8,1]
print(Y1)
# (3) or use fitted function
Z <- fitted(m)
Z[8]

# (4)
Y2 <- c[1] + c[2]*pr27$age
sum((pr27$mass-Y2)^2)
```

1.28
    (a) Generally, the estimated regression function appeard as a good fit and showed a negative relationship between the level of education and crime rate in medium-sized U.S. countries. It seems like there are larger variability of erros across the x values compared to variability of the previous data set(1.27), but below Q-Q plot showed that the variability across the x values is constant. The residuals were also distributed evenly or symmetrically, which indicates mean of erros at a given x value is zero.
  
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR28.txt", destfile="CH01PR28.txt")
pr28 = read.table("CH01PR28.txt", col.names=c("Crime_rate","Percent"))
m1 <- lm(Crime_rate~Percent, data=pr28)
summary(m1)

# Check linearilty of model
plot(pr28$Percent,pr28$Crime_rate,xlab="% of individual with at least highschool diploma",ylab="Crime rate")
abline(m1)

# Verity the linearilty as well as constant variability
plot(m1$residuals ~ pr28$Percent)
abline(h = 0, lty = 3)

# Check distribution of residuals
qqnorm(m1$residuals)
qqline(m1$residuals)

```

  (b)
    (1)

```{r}
residuals <- resid(m1)
head(residuals)

```

    (2) at X=80, (3) at 10th case of the X, (4) 455,273,165
```{r}
c1 <- coef(m1)
# (2) at age 60yr
Y2 = c1[1] + c1[2]*60
print(Y2)

# (3) use fitted function
Z1 <- fitted(m1)
Z1[10]

# (4)
Y3 <- c1[1] + c1[2]*pr28$Percent
sum((pr28$Crime_rate-Y3)^2)
```

1.36 sum(Yhat*error)
     => sum of error == zero (1.17)
     => sum of X*error = zero (1.19)
     => Thus, sum of Yhat*error also have to be zero

1.42
    a. When B1 = 17, 18, 19
    b. B1 = 18, the likelihood function is largest
    c. (B) is consistent with (c) B1=17.93 if we can round the B1 value from (c)
    d. I am not sure how to do it. Could you give me feedback on this?
  
```{r}   
  B1 <- as.matrix(c(17,18,19))
  x_142 <- as.matrix(c(7,12,4,14,25,30))
  u_142 <- as.matrix(c(128,213,75,250,446,540))
  y_142 <- matrix(0,6,1) 
  
  k <- 0 
  max_likely <- matrix(0,3,1)
  f <- matrix(0,6,1)

for (j in 1:3) {
  
  y_142 <- B1[j,1] * x_142 
  
  for (i in 1:nrow(x_142)){

    # State of likelihood function at a given X
    k <- ((u_142[i,1] - y_142[i,1])/4)^2
    f[i,1] <- (1/(sqrt(2*3.14)*4))*exp((-1/2)*k)
    
  }
  
  max_likely[j,1] <- f[1,1]*f[2,1]*f[3,1]*f[4,1]*f[5,1]*f[6,1]
    
}

#(b)When B1 = 18, the likelihood function is largest(second row value came out)
max(max_likely)

# (c) When B1 = 17.93, the likelihood function is largest
b1_estimator <- sum(x_142*u_142)/sum(x_142^2)

```

Chapter 6

6.2
    a. Matrix expression when i = 1~5
    
```{r}
# Y = Xb
Y <- as.matrix(c("Y1","Y2","Y3","Y4","Y5"))
X <- matrix(c(1,1,1,1,1,"X11","X21","X31","X41","X51","x12","x22","x32","x42",
         "x52","xx13","xx23","xx33","xx43","xx53"), nrow=5)
b <- matrix(c("0","b1","b2","b3"))
Y
X
b
```

  b. ??
  
6.3

    Adding predictor variables could cause increase in squared R value because at least small variance of data can be explained by the variables. owever, we don't know whether adding those variables leads to significant increase in squared R value or not. 
    
6.5
  a. scatter plot matrix and correlation matrix
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%206%20Data%20Sets/CH06PR05.txt", destfile="CH06PR05.txt")
pr05 = read.table("CH06PR05.txt", col.names=c("y","x1", "x2"))
# Scatter plot
plot(pr05[1:3])
pairs(pr05)

# Correlation matrix
as.matrix(cor(pr05))

```

    b. Fit regression model (6,1)
```{r}
multi_pr05 <- lm(formula = y ~ x1 + x1*x2, data = pr05)

```
    c. Residuals and a box plot of the residuals. The box plot showed 25 and 75 percentiles as well as median. Thus, we can imagine distribution of the residuals.

```{r}
# Residuals
summary(multi_pr05)

# Box plot
boxplot(multi_pr05$residuals)

```

  d. residuals against
  
```{r}
# Residuals against y(hat), recheck
#co <- multi_pr05$coef

#plot(y_hat, multi_pr05$residuals)


# Residuals against x1
plot(pr05$x1, multi_pr05$residuals)

# Residuals against x2
plot(pr05$x2, multi_pr05$residuals)

```

  d. Normal probability plot
```{r}
qqnorm(multi_pr05$residuals)
qqline(multi_pr05$residuals)
```

6.9
    a. There were no gaps between te data. 
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%206%20Data%20Sets/CH06PR09.txt", destfile="CH06PR09.txt")
pr09 = read.table("CH06PR09.txt", col.names=c("y","x1", "x2","x3"))
stem(pr09$x1)
stem(pr09$x2)
```

    b.Below plots showed how each variable changes as a function of time (Unit: week). The variable y and x2 showed fluctuation across the times while the variable x1 showed higher numbers at the end of the year. 
```{r}
x <- c(1:nrow(pr09))
plot(x,pr09$y, type='l',xlab="Time(week)", ylab="Total Labor hours")
plot(x,pr09$x1, type='l',xlab="Time(week)", ylab="# of cases shipped")
plot(x,pr09$x2, type='l',xlab="Time(week)", ylab="Indirect cost(%)")
plot(x,pr09$x3, xlab="Time(week)", ylab="Week(no holiday=0)")
```

    c.We can check correlation values and relationship between all possible combinations of two variables. The total labor hours were larger when a week has holiday (r=0.81) while the other r values were very low. 
```{r}
# Scatter plot
pairs(pr09)

# Correlation matrix
as.matrix(cor(pr09))
```


6.10
    a. fit regression model
    y = 456.8 - 0.0005*x1 - 67.86*x2 + 619*x3 -0.00017*x1*x2
    The variable x1, x2, and interaction between x1 and x2 didn't show sigfinicant relationship with the variable y. Here, b1 indicated that a one unit increase in x1 decreases -0.0005 of y value while the other variables are fixed or were not changed. We can interpret b2 and b3 each for x2 and x3 by the same way.
```{r}
multi_pr09 <- lm(formula = y ~ x1 + x2 + x3 + x1*x2, data = pr09)
summary(multi_pr09)

```
    b. a box plot : Theboxplot looks like a normal distribution with zero mean and symmatrical shape. Thus, the regression results showed that the regression model didn't violate two assumptions: 1) summation of error is zero and 2) normal distribution. 
```{r}
boxplot(multi_pr09$residuals)
```

  d. The errors was not correlated each other.
```{r}
plot(x, multi_pr09$residuals)
```

6.15
  a.
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%206%20Data%20Sets/CH06PR15.txt", destfile="CH06PR15.txt")
pr15 = read.table("CH06PR15.txt", col.names=c("y","x1", "x2","x3"))

```
  a stem and leaf plot for each of the predictor variables
```{r}
stem(pr15$y)
stem(pr15$x1)
stem(pr15$x2)
stem(pr15$x3)
```

    b. Each of x1, x2, and x3 variable showed negative correlation with y variable. We can clearly look at the relationship from below scatter plot and correlation matrix. 
```{r}
# Scatter plot
pairs(pr15)

# Correlation matrix
as.matrix(cor(pr15))
```

    c.b2 indicated that a one unit increase in x2 decreases -0.442 of y value while the other variables are fixed or were not changed.
```{r}
multi_pr15 <- lm(formula = y ~ x1 + x2 + x3, data = pr15)
summary(multi_pr15)
```

    d. There were no outliers among the residuals.
```{r}
boxplot(multi_pr15$residuals)
```

6.31
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Appendix%20C%20Data%20Sets/APPENC01.txt", destfile="APPENC01.txt")
appenc01 = read.table("APPENC01.txt", col.names=c("ID","Lstay", "Age","Irisk",
                                              "RCratio","RXratio","Nbed",
                                              "Msaffil","Region","AveCensus",
                                              "Nnurse","FandS"))
```
  a. 
```{r}
# X1 : age, X2 : routine culturing ratio, X3: averageCensus, X4 : FandS
# For each region

multi_r1 <- lm(formula = Irisk ~ Age+RCratio+AveCensus+FandS, data = subset(appenc01, appenc01$Region==1))
multi_r2 <- lm(formula = Irisk ~ Age+RCratio+AveCensus+FandS, data = subset(appenc01, appenc01$Region==2))
multi_r3 <- lm(formula = Irisk ~ Age+RCratio+AveCensus+FandS, data = subset(appenc01, appenc01$Region==3))
multi_r4 <- lm(formula = Irisk ~ Age+RCratio+AveCensus+FandS, data = subset(appenc01, appenc01$Region==4))

summary(multi_r1)
summary(multi_r2)
summary(multi_r3)
summary(multi_r4)
```
    b.
    Region 1: y=-3.35 + 0.12*age + 0.0582*RCratio + 0.002*Census + 0.007*FandS
    Region 2: y= 2.29 + 0.005*age + 0.058*RCratio + 0.001*Census + 0.015*FandS
    Region 3: y=-0.14 + 0.03*age + 0.102*RCratio + 0.004*Census + 0.008*FandS
    Region 4: y= 1.57 + 0.04*age + 0.0588*RCratio - 0.0007*Census + 0.013*FandS
  
    The regression lines look similar each other except different intercept for region 3 and slope of Census for region 4. 

    c. For each region, see below. Based on the result, the MSE values across the regions were similar each other while the squared R values were different. Suared R value for the region 3 was larger than one from the other regions. Thus, the X variables explain more variance of the region 3. 

```{r}  
## Calculation of MSE and R^2 for each region
# Function to calculate MSE and R^2
mse_r <- function(data,n){
  
  region <- subset(data, data$Region==n)
  
  x_mat <- cbind(c(1),region$Age,region$RCratio,region$AveCensus,region$FandS)
# Calculation of coefficient
  b_mat <- solve((t(x_mat) %*% x_mat)) %*% t(x_mat) %*% region$Irisk

# Calculation of SSE
  sse<-t(region$Irisk)%*%region$Irisk - t(b_mat)%*% t(x_mat)%*%region$Irisk
  mse <- sse/(nrow(region)-5)

# Calculation of SSTO and R^2
  j <- matrix(1,nrow(region),nrow(region))
  ssto <- t(region$Irisk)%*%region$Irisk - (1/nrow(region)) * (t(region$Irisk)%*%j%*%region$Irisk)

  sqR <- 1 - (sse/ssto)
  
  return(cbind(mse, sqR))
}

region1 <- mse_r(appenc01,1)
region2 <- mse_r(appenc01,2)
region3 <- mse_r(appenc01,3)
region4 <- mse_r(appenc01,4)
result <- rbind(region1, region2, region3, region4)
colnames(result) <- c("MSE","R^2")
rownames(result) <- c("Region1","Region2","Region3","Region4")

# Show result
result
```  


    d. Box plot for residuals. In region 3, there were 3 outliers in the boxplot of the residuals while the other boxplots didn't have any outliers. 
  
```{r}  
boxplot(multi_r1$residuals,main="Region 1")
boxplot(multi_r2$residuals,main="Region 2")
boxplot(multi_r3$residuals,main="Region 3")
boxplot(multi_r4$residuals,main="Region 4")

``` 
