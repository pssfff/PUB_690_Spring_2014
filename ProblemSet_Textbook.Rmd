Problem Set 1
Due date : 2.13.2014

========================================================
1.5
  - I don't agree with it because one of the assumptions was that summation of error at a given X value should be zero. Thus, we can simplify more for the equation by removing the error term.
  
1.6
  (a) 
  
```{r}
B_0 <- 200
B_1 <- 5

# Recheck 3D plot
X <- seq(from=0,to=50,by=10)
Y <- B_0 + B_1 * X
plot(X,Y,type="l")
```
  
  (b) Parameter β0 : expected Y value when X value is zero (X=0)
  
      Parameter β1 : the amount of change in Y value when a unit of X value increases

1.7
  (a) β0 = 100, β1 = 20 => Y = 100 + 20 * X
  
      Expected Y value is 200 when the X value is 5,
      variance of the expected Y value is 25
      => We don't know what distribution the error term has because it is unspecified.
      
  (b) The range from 195 to 205 indicates 1*standard deviation from the mean. This suggests that the probability will be around 68% if we assume that error has normal distribution.
  
1.10
  - The curvilinear relationshp indicates that changes in Salary at changes in relatively young age is larger than ones at changes in relativley old age. The slopes of the curvilinear relationship are postivie regardless of X values (here, ages). That is, the salary increases as the age increases until ages reaches 47 years.
  
1.11 Recheck!
  - There is a linear relationship between the two variables, outputs from pre- and post-training. We know that outputs after post-training is less than ones before pre-training. However, we can't know causation. There might be different reasons to lead to the result. 
  
1.27
  (a) The linear regression function below fit the data points well.We can see symmetrical distribution of residuals and constant variability of the residuals arcoss x values (See Normal Q-Q Plot below). The negative correlation indicated that muscle mass decreases with age.
  
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR27.txt", destfile="CH01PR27.txt")
pr27 = read.table("CH01PR27.txt", col.names=c("age", "mass"))
m <- lm(mass~age, data=pr27)
summary(m)
# Check linearilty of model
plot(pr27$age,pr27$mass,xlab="Age",ylab="Muscle Mass")
abline(m)

# Verity the linearilty as well as constant variability
plot(m$residuals ~ pr27$age)
abline(h = 0, lty = 3)

# Check distribution of residuals
qqnorm(m$residuals)
qqline(m$residuals)
```

  (b)
    (1) residuals at change in age by one year

```{r}
residuals <- resid(m)
head(residuals)

```

    (2) at X=60, (3) at 8th case of the X, (4) 5.948^2= 35.383  
```{r}
c <- coef(m)
# (2) at age 60yr
Y = c[1] + c[2]*60
print(Y)

# (3) expected Y value at 8th case of the X
Y1 = c[1] + c[2]*pr27[8,1]
print(Y1)
# (3) or use fitted function
Z <- fitted(m)
Z[8]

```

1.28
  (a) Generally, the estimated regression function appeard as a good fit and showed a negative relationship between the level of education and crime rate in medium-sized U.S. countries. It seems like there are larger variability of erros across the x values compared to variability of the previous data set(1.27), but below Q-Q plot showed that the variability across the x values is constant. The residuals were also distributed evenly or symmetrically, which indicates mean of erros at a given x value is zero.
  
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR28.txt", destfile="CH01PR28.txt")
pr28 = read.table("CH01PR28.txt", col.names=c("Percent", "Crime_rate"))
m1 <- lm(Crime_rate~Percent, data=pr28)
summary(m1)

# Check linearilty of model
plot(pr28$Percent,pr28$Crime_rate,xlab="% of individual with at least highschool diploma",ylab="Crime rate")
abline(m1)

# Verity the linearilty as well as constant variability
plot(m1$residuals ~ pr28$Percent)
abline(h = 0, lty = 3)

# Check distribution of residuals
qqnorm(m1$residuals)
qqline(m1$residuals)

```

  (b)
    (1)

```{r}
residuals <- resid(m1)
head(residuals)

```

    (2) at X=80, (3) at 10th case of the X, (4) 5.701^2 = 32.5014
```{r}
c1 <- coef(m1)
# (2) at age 60yr
Y2 = c1[1] + c1[2]*80
print(Y2)

# (3) use fitted function
Z1 <- fitted(m1)
Z1[10]
```

1.36 ??

1.42
  a. When B1 = 17, 18, 19
  b. B1 = 18, the likelihood function is largest
  c. (B) is consistent with (c) B1=17.93 if we can round the B1 value from (c)
  d. ???
  
```{r}   
  B1 <- as.matrix(c(17,18,19))
  x_142 <- as.matrix(c(7,12,4,14,25,30))
  u_142 <- as.matrix(c(128,213,75,250,446,540))
  y_142 <- matrix(0,6,1) 
  
  k <- 0 
  max_likely <- matrix(0,3,1)
  f <- matrix(0,6,1)

for (j in 1:3) {
  
  y_142 <- B1[j,1] * x_142 
  
  for (i in 1:nrow(x_142)){

    # State of likelihood function at a given X
    k <- ((u_142[i,1] - y_142[i,1])/4)^2
    f[i,1] <- (1/(sqrt(2*3.14)*4))*exp((-1/2)*k)
    
  }
  
  max_likely[j,1] <- f[1,1]*f[2,1]*f[3,1]*f[4,1]*f[5,1]*f[6,1]
    
}

# (b) When B1 = 18, the likelihood function is largest
max(max_likely)

# (c) When B1 = 17.93, the likelihood function is largest
b1_estimator <- sum(x_142*u_142)/sum(x_142^2)

```
Chapter 2

2.1 Recheck!
  a. The student can't conclude that there is a linear association between Y and X because the estimated value of slope is falling between the 95 percent confidence limits. Thus, we can't reject the null hypothesis that B1 is zero. 
  
  b. Yes, the native value couldn't give us valuable information, so we might use the centering, which shifts the regression up. It doesn't affect the slope at all.
  
2.10
  a.
  b.
  c.
  
2.13
  a.

```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR19.txt", destfile="CH01PR19.txt")
pr19 = read.table("CH01PR19.txt", col.names=c("GPA", "ACT"))
m2 <- lm(GPA~ACT, data=pr19)
summary(m2)
# Check linearilty of model
plot(pr19$ACT,pr19$GPA,xlab="ACT",ylab="GPA")
abline(m2)

# at 28 (ACT)
cc <- coef(m2)
YY <- c[1] + c[2] * 28
print(YY)

# To calculate intervals, S^2 = MSE[~]
n <- 120
MSE <- sum(m2$residuals^2)/(n-2)
ave_x <- mean(pr19$ACT)
ss_2_y_h <- MSE * ((1/n) + ((28 - ave_x)^2)/sum((pr19$ACT - ave_x)^2))
ss <- sqrt(ss_2_y_h)

# t(0.975; 118) = 2.3583
upper_b <- YY + 2.3583*ss
under_b <- YY - 2.3583*ss

# 95% confidential interval at X = 60, upper and under bound
upper_b
under_b

```
  b. predication of the new value 
```{r}  
# s^2{pred} = MSE + S^2{Y_hat_h}
ss_2_pred <- MSE + ss_2_y_h
ss_pred <- sqrt(ss_2_pred)
upper_b_2 <- YY + 2.3583 * ss_pred
under_b_2 <- YY - 2.3583 * ss_pred
# 95% confidential interval at new prediction, upper and under bound
upper_b_2
under_b_2

```
  c.(b) is wider (c)
    - 
  d. 
- The confidence band is wider than (a)
- Yes, according to the textbook (p61), the confidence band represents the entire area where all possible regression lines are located. Thus, the confidence band must include the confidence interval from (a)
```{r}
# W^2 = 2F(0.95;2,118) 
w_2 <- 2*qf(0.95, df1=2,df2=118)
w <- sqrt(w_2)
band_up <- YY + w * ss
band_un <- YY - w * ss

# band 
band_up
band_un
```
  
  
2.28

  a.
```{r}  
# m => include lm results for the age and mass data set
summary(m)

c <- coef(m)
# at age 60yr
Y = c[1] + c[2]*60
print(Y)

# To calculate intervals, S^2 = MSE[~]
n <- 60
MSE <- sum(m$residuals^2)/(n-2)
ave_x <- mean(pr27$age)
s_2_y_h <- MSE * ((1/n) + ((60 - ave_x)^2)/sum((pr27$age - ave_x)^2))
s <- sqrt(s_2_y_h)

# t(0.975; 58) = 2.00172
upper_b <- Y + 2.00172*s
under_b <- Y - 2.00172*s

# 95% confidential interval at X = 60, upper and under bound
upper_b
under_b
```

b.
```{r}  
# s^2{pred} = MSE + S^2{Y_hat_h}
s_2_pred <- MSE + s_2_y_h
s_pred <- sqrt(s_2_pred)
upper_b_2 <- Y + 2.00172 * s_pred
under_b_2 <- Y - 2.00172 * s_pred
# 95% confidential interval at new prediction, upper and under bound
upper_b_2
under_b_2

```
c. 
- The confidence band is wider than (a)
- Yes, according to the textbook (p61), the confidence band represents the entire area where all possible regression lines are located. Thus, the confidence band must include the confidence interval from (a)
```{r}
# W^2 = 2F(0.95;2,58)
w_2 <- 2*3.15593
w <- sqrt(w_2)
band_up <- Y + w * s
band_un <- Y - w * s

# band 
band_up
band_un
```

2.30

  a.In below summary, we can know there is a significant relationship between the two variables based on t-value as well as p-value(p < 0.001).
  
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR28.txt", destfile="CH01PR28.txt")
pr28 = read.table("CH01PR28.txt", col.names=c("Percent", "Crime_rate"))
m1 <- lm(Crime_rate~Percent, data=pr28)
summary(m1)
```
  b. The slope is between the 99% confidence interval. This indicates that we can 99% sure that a slope of a true regression line is falling into the confidence interval.
  
```{r}
n <- 84
MSE <- sum(m1$residuals^2)/(n-2)
x_ave <- mean(pr28$Percent)
s_2_b1 <- MSE/sum((pr28$Percent - x_ave)^2)
s_b1 <- sqrt(s_2_b1)
# t(0.995;82)
tt <- qt(0.995,df=82)
b1_up <- m1$coef[2] + tt*s_b1
b1_un <- m1$coef[2] - tt*s_b1

b1_up
b1_un
```

2.50 ?


2.68 ?

