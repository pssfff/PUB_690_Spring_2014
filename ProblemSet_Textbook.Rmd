Problem Set 1
Due date : 2.13.2014

========================================================
1.5
  - I don't agree with it because one of the assumptions was that summation of error at a given X value should be zero. Thus, we can simplify more for the equation by removing the error term.
  
1.6
  (a) 
  
```{r}
B_0 <- 200
B_1 <- 5

# Recheck 3D plot
X <- seq(from=0,to=50,by=10)
Y <- B_0 + B_1 * X
plot(X,Y,type="l")
```
  
  (b) Parameter β0 : expected Y value when X value is zero (X=0)
  
      Parameter β1 : the amount of change in Y value when a unit of X value increases

1.7
  (a) β0 = 100, β1 = 20 => Y = 100 + 20 * X
  
      Expected Y value is 200 when the X value is 5,
      variance of the expected Y value is 25
      => We don't know what distribution the error term has because it is unspecified.
      
  (b) The range from 195 to 205 indicates 1*standard deviation from the mean. This suggests that the probability will be around 68% if we assume that error has normal distribution.
  
1.10
  - The curvilinear relationshp indicates that changes in Salary at changes in relatively young age is larger than ones at changes in relativley old age. The slopes of the curvilinear relationship are postivie regardless of X values (here, ages). That is, the salary increases as the age increases until ages reaches 47 years.
  
1.11 Recheck!
  - There is a linear relationship between the two variables, outputs from pre- and post-training. We know that outputs after post-training is less than ones before pre-training. However, we can't know causation. There might be different reasons to lead to the result. 
  
1.27
  (a) The linear regression function below fit the data points well.We can see symmetrical distribution of residuals and constant variability of the residuals arcoss x values (See Normal Q-Q Plot below). The negative correlation indicated that muscle mass decreases with age.
  
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR27.txt", destfile="CH01PR27.txt")
pr27 = read.table("CH01PR27.txt", col.names=c("age", "mass"))
m <- lm(mass~age, data=pr27)
summary(m)
# Check linearilty of model
plot(pr27$age,pr27$mass,xlab="Age",ylab="Muscle Mass")
abline(m)

# Verity the linearilty as well as constant variability
plot(m$residuals ~ pr27$age)
abline(h = 0, lty = 3)

# Check distribution of residuals
qqnorm(m$residuals)
qqline(m$residuals)
```

  (b)
    (1) residuals at change in age by one year

```{r}
residuals <- resid(m)
head(residuals)

```

    (2) at X=60, (3) at 8th case of the X, (4) 5.948^2= 35.383  
```{r}
c <- coef(m)
# (2) at age 60yr
Y = c[1] + c[2]*60
print(Y)

# (3) expected Y value at 8th case of the X
Y1 = c[1] + c[2]*pr27[8,1]
print(Y1)
# (3) or use fitted function
Z <- fitted(m)
Z[8]

```

1.28
  (a) Generally, the estimated regression function appeard as a good fit and showed a negative relationship between the level of education and crime rate in medium-sized U.S. countries. It seems like there are larger variability of erros across the x values compared to variability of the previous data set(1.27), but below Q-Q plot showed that the variability across the x values is constant. The residuals were also distributed evenly or symmetrically, which indicates mean of erros at a given x value is zero.
  
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR28.txt", destfile="CH01PR28.txt")
pr28 = read.table("CH01PR28.txt", col.names=c("Crime_rate","Percent"))
m1 <- lm(Crime_rate~Percent, data=pr28)
summary(m1)

# Check linearilty of model
plot(pr28$Percent,pr28$Crime_rate,xlab="% of individual with at least highschool diploma",ylab="Crime rate")
abline(m1)

# Verity the linearilty as well as constant variability
plot(m1$residuals ~ pr28$Percent)
abline(h = 0, lty = 3)

# Check distribution of residuals
qqnorm(m1$residuals)
qqline(m1$residuals)

```

  (b)
    (1)

```{r}
residuals <- resid(m1)
head(residuals)

```

    (2) at X=80, (3) at 10th case of the X, (4) 5.701^2 = 32.5014
```{r}
c1 <- coef(m1)
# (2) at age 60yr
Y2 = c1[1] + c1[2]*60
print(Y2)

# (3) use fitted function
Z1 <- fitted(m1)
Z1[10]
```

1.36 ??

1.42
  a. When B1 = 17, 18, 19
  b. B1 = 18, the likelihood function is largest
  c. (B) is consistent with (c) B1=17.93 if we can round the B1 value from (c)
  d. I am not sure how to do it. Could you give me feedback on this?
  
```{r}   
  B1 <- as.matrix(c(17,18,19))
  x_142 <- as.matrix(c(7,12,4,14,25,30))
  u_142 <- as.matrix(c(128,213,75,250,446,540))
  y_142 <- matrix(0,6,1) 
  
  k <- 0 
  max_likely <- matrix(0,3,1)
  f <- matrix(0,6,1)

for (j in 1:3) {
  
  y_142 <- B1[j,1] * x_142 
  
  for (i in 1:nrow(x_142)){

    # State of likelihood function at a given X
    k <- ((u_142[i,1] - y_142[i,1])/4)^2
    f[i,1] <- (1/(sqrt(2*3.14)*4))*exp((-1/2)*k)
    
  }
  
  max_likely[j,1] <- f[1,1]*f[2,1]*f[3,1]*f[4,1]*f[5,1]*f[6,1]
    
}

# (b) When B1 = 18, the likelihood function is largest
max(max_likely)

# (c) When B1 = 17.93, the likelihood function is largest
b1_estimator <- sum(x_142*u_142)/sum(x_142^2)

```
Chapter 2

2.1 
  a. The student can't conclude that there is a linear association between Y and X because the estimated value of slope is falling between the 95 percent confidence limits. Thus, we can't reject the null hypothesis that B1 is zero. 
  
  b. Yes, the native value couldn't give us valuable information, so we might use the centering, which shifts the regression up. It doesn't affect the slope at all.
  
2.10
  a.
  b.
  c.
  
2.13
  a.

```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR19.txt", destfile="CH01PR19.txt")
pr19 = read.table("CH01PR19.txt", col.names=c("GPA", "ACT"))
m2 <- lm(GPA~ACT, data=pr19)
summary(m2)
# Check linearilty of model
plot(pr19$ACT,pr19$GPA,xlab="ACT",ylab="GPA")
abline(m2)

# at 28 (ACT)
cc <- coef(m2)
YY <- c[1] + c[2] * 28
print(YY)

# To calculate intervals, S^2 = MSE[~]
n <- 120
MSE <- sum(m2$residuals^2)/(n-2)
ave_x <- mean(pr19$ACT)
ss_2_y_h <- MSE * ((1/n) + ((28 - ave_x)^2)/sum((pr19$ACT - ave_x)^2))
ss <- sqrt(ss_2_y_h)

# t(0.975; 118) = 2.3583
upper_b <- YY + 2.3583*ss
under_b <- YY - 2.3583*ss

# 95% confidential interval at X = 60, upper and under bound
upper_b
under_b

```
  b. predication of the new value 
```{r}  
# s^2{pred} = MSE + S^2{Y_hat_h}
ss_2_pred <- MSE + ss_2_y_h
ss_pred <- sqrt(ss_2_pred)
upper_b_2 <- YY + 2.3583 * ss_pred
under_b_2 <- YY - 2.3583 * ss_pred
# 95% confidential interval at new prediction, upper and under bound
upper_b_2
under_b_2

```
  c.(b) is wider than (c)
    
  d. 
- The confidence band is wider than (a)
- Yes, according to the textbook (p61), the confidence band represents the entire area where all possible regression lines are located. Thus, the confidence band must include the confidence interval from (a)
```{r}
# W^2 = 2F(0.95;2,118) 
w_2 <- 2*qf(0.95, df1=2,df2=118)
w <- sqrt(w_2)
band_up <- YY + w * ss
band_un <- YY - w * ss

# band 
band_up
band_un
```
  
  
2.28

  a.
```{r}  
# m => include lm results for the age and mass data set
summary(m)

c <- coef(m)
# at age 60yr
Y = c[1] + c[2]*60
print(Y)

# To calculate intervals, S^2 = MSE[~]
n <- 60
MSE <- sum(m$residuals^2)/(n-2)
ave_x <- mean(pr27$age)
s_2_y_h <- MSE * ((1/n) + ((60 - ave_x)^2)/sum((pr27$age - ave_x)^2))
s <- sqrt(s_2_y_h)

# t(0.975; 58) = 2.00172
upper_b <- Y + 2.00172*s
under_b <- Y - 2.00172*s

# 95% confidential interval at X = 60, upper and under bound
upper_b
under_b
```

b.
```{r}  
# s^2{pred} = MSE + S^2{Y_hat_h}
s_2_pred <- MSE + s_2_y_h
s_pred <- sqrt(s_2_pred)
upper_b_2 <- Y + 2.00172 * s_pred
under_b_2 <- Y - 2.00172 * s_pred
# 95% confidential interval at new prediction, upper and under bound
upper_b_2
under_b_2

```
c. 
- The confidence band is wider than (a)
- Yes, according to the textbook (p61), the confidence band represents the entire area where all possible regression lines are located. Thus, the confidence band must include the confidence interval from (a)
```{r}
# W^2 = 2F(0.95;2,58)
w_2 <- 2*3.15593
w <- sqrt(w_2)
band_up <- Y + w * s
band_un <- Y - w * s

# band 
band_up
band_un
```

2.30

  a.In below summary, we can know there is a significant relationship between the two variables based on t-value as well as p-value(p < 0.001).
  
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR28.txt", destfile="CH01PR28.txt")
pr28 = read.table("CH01PR28.txt", col.names=c("Crime_rate","Percent"))
m1 <- lm(Crime_rate~Percent, data=pr28)
summary(m1)
```
  b. The slope is between the 99% confidence interval. This indicates that we can 99% sure that a slope of a true regression line is falling into the confidence interval.
  
```{r}
n <- 84
MSE <- sum(m1$residuals^2)/(n-2)
x_ave <- mean(pr28$Percent)
s_2_b1 <- MSE/sum((pr28$Percent - x_ave)^2)
s_b1 <- sqrt(s_2_b1)
# t(0.995;82)
tt <- qt(0.995,df=82)
b1_up <- m1$coef[2] + tt*s_b1
b1_un <- m1$coef[2] - tt*s_b1

b1_up
b1_un
```

2.50 What is the ki ?


2.68 

  a. Linear regression line below
```{r}

download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR20.txt", destfile="CH01PR20.txt")
pr20 = read.table("CH01PR20.txt", col.names=c("Min","Copier"))
m3 <- lm(Min~Copier, data=pr20)
summary(m3)
plot(pr20$Copier,pr20$Min,xlab="Copier",ylab="Min", xlim=c(0,8))
axis(side=1, at=c(1:8))
abline(m3)

```

  b. Please look at the red lines in the first plot. The true regression line exists betwee the confidential interval (90%). Thus, the interval estimated the true one accurately.
  
```{r}
# Hand calculation to add confidential intervals for the true regression
n <- 45
MSE <- sum(m3$residuals^2)/(n-2)
ave_x <- mean(pr20$Copier)
xh <- as.matrix(c(1:8))
ss_2 <- matrix(0,8,1)

for (i in 1:8) {
  
  ss_2[i,1] <- MSE*((1/n)+((xh[i,1] - ave_x)^2)/sum((pr20$Copier-ave_x)^2))
  
}

ss <- sqrt(ss_2)

# W^2 = 2F(0.95;2,58)
w_2 <- 2 * qf(0.90,2,43)
w <- sqrt(w_2)
y <- m3$coef[1] + m3$coef[2] * xh
band_up <- y + w * ss
band_un <- y - w * ss

plot(pr20$Copier,pr20$Min,xlab="Copier",ylab="Min", xlim=c(0,8))
axis(side=1, at=c(1:8))
abline(m3)
lines(band_up, col="red")
lines(band_un, col="red")

# Using qplot for the true regression line
library(ggplot2)
qplot(Copier, Min, data=pr20, geom=c("point", "smooth"),method="lm")+ theme_bw()

```

Chapter 3.

3.8 
  a. Stem plot for X : distribtuion of the variable
  
```{r}
download.file("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR28.txt", destfile="CH01PR28.txt")
pr28 = read.table("CH01PR28.txt", col.names=c("Crime_rate","Percent"))
m1 <- lm(Crime_rate~Percent, data=pr28)
# stem and leaf plot for X
stem(pr28$Percent)

```

  b. Yes, the distribution looks like symmetrical.
```{r}
# Stem and leaf plot for residuals across X
stem(m1$residuals)

```

  c. Residual plot of residuals versus Y. As the crime rate increases, the residuals also increase. 
```{r}
# Residual plot of error versus expected Y

y <- m1$coef[1] + m1$coef[2] * pr28$Percent
plot(y, m1$residuals)
```

  d. Normal probability plot of the residuals. The correlation coefficient is close to 1. Under alpha = 0.05 level, the coefficient (0.989) is larger than 0.985~0.986(Table B.6). Thus, we can conclude that the distrubtion of the residuals is not significantly different from the normali distribution and the model didn't violate one of assumptions that regression analysis has.
  
```{r}
# Check distribution of residuals
qqnorm(m1$residuals)
qqline(m1$residuals)

# Obtain the coefficient of correlation bet ordered residuals and expected
# Followed Textbook p115
n <- 84
MSE <- sum(m1$residuals^2)/(n-2)
root_MSE <- sqrt(MSE)
m1_resi <- sort(m1$residuals, decreasing=FALSE)
expected_resi <- matrix(0,n,1)
# Calculate expected residuals
for (i in 1:length(m1$residuals)) {
  expected_resi[i,] <- root_MSE * qnorm(((i-0.375)/(n+0.25)), mean=0, sd=1)
}

# Correlation coefficient
cor(m1_resi, expected_resi)
plot(m1_resi, expected_resi)


```
    Based on Table 3.6, lack of fit test
    => The regression line is linear (Stick to Null Hypo)
```{r}
# First soring by Y values at the same X
new_pr28 <- pr28[order(pr28$Percent, xtfrm(pr28$Percent)), ]
# Calculate SSPE sum of (Y - mean(Y))^2
pp <- tapply(new_pr28$Crime_rate, factor(new_pr28$Percent), function (x) sum((x - mean(x))^2))
sspe <- sum(pp)
n_sspe <- length(pr28$Percent)
c_sspe <- nlevels(factor(x$Percent))
df_sspe <- n_sspe - c_sspe

# Linear regression line given factor
y <- m1$coef[1] + m1$coef[2] * pr28$Percent

# Calculation of SSE
sse <- sum((pr28$Crime_rate-y)^2)

# F* = sslf/c-2 / sspe/n-c
sslf = sse - sspe

f_stat <- (sslf/(c_sspe-2))/(sspe/(df_sspe))
f_conf <- qf(0.95,c_sspe-2,df_sspe)
if (f_stat > f_conf){
  print("Reject Null Hypo")
}else{
  print("Stick to Null Hypo = The regression line is linear")
}

```

  e. Brown-Forsythe Test (non dependent on normality of error term) is usually used for a simple linear regression in which variacne of error terms either increase or decrease with X. Based on below result, variances of the error from the two groups were not different. This indicated that residuals or errors have constant variability across X values. This result was also consistent with the result from (c). The residual plot in (c) showed fluctuations around zero. 
  
```{r}
# divide the data set into two groups
# First 8 rows (<=69) and 78 (>69) in the "new_pr28" dataset
new_pr28 <- pr28[order(pr28$Percent, xtfrm(pr28$Percent)), ]
mat_new_pr28 <- as.matrix(new_pr28)
grp1_pr28 <- mat_new_pr28[1:8,]
grp2_pr28 <- mat_new_pr28[9:84,]

# Absolute residuals
resi_grp1 <- grp1_pr28 - mean(grp1_pr28)
resi_grp2 <- grp2_pr28 - mean(grp2_pr28)

# residuals calculated from median, d1 and d2
grp1_d = abs(resi_grp1 - median(resi_grp1))
d1 <- mean(grp1_d)
grp2_d = abs(resi_grp2 - median(resi_grp2))
d2 <- mean(grp2_d)
# Pooled variance
s_2 <- (sum((grp1_d-d1)^2) + sum((grp2_d-d2)^2))/(82)
sss <- sqrt(s_2)
# t-statistics
t_stat <- (d1-d2)/(sss*sqrt(1/8 + 1/76))

# 95% confidential level
t_p_stat <- qt(0.975, 82)

if (t_stat <= t_p_stat){
  
  print("Variance of the error is constant")
  
}else{
  
  print("Variance of the error is not constant")
}

```
  
3. 19
  - The different could originate from outliers. The linear regression represents a line that minimizes residuals between predicted Y variable and the explanatory variable (X), so that the residuals might be more oscillating around zero compared to residuals that the original data showed. Even though the linear regression model didn't violate assumption on constant variance of residuals in its prediction, we have to think of variance that the original data set has. Thus, we might need to check whether the original data set has outliers. 


3.24
  a. Case 7 looks like an outlier because the data point is relatively far away from the zero line. 
  
```{r}
# Input data
x_var <- c(5,8,11,7,13,12,12,6)
y_var <- c(63,67,74,64,76,69,90,60)

press_data <- data.frame(x_var,y_var)
# regression
m4 <- lm(y_var ~ x_var, data=press_data)

plot(x_var, m4$residuals)

```

  b. Coefficient of determination was increased and residual standard error was decreased by removing the 7th data. 

```{r}
# Remove the 7th row
new_press_data <- press_data[-7,]
m5 <- lm(y_var ~ x_var, data=new_press_data)
summary(m5)
summary(m4)
```

  c. X = 12, 99% confidential interval is from 66.75 to 79.02. The Y value, 90 at X=12 is not falling in the interval. This data point could be considered as an outlier based on the 99% confidential inteval and the point could affect p-value that is used when we check that a linear regression model is fitted into data set or not. Thus, we must care about how to deal with outliers when we analyze our own data set. 
  
```{r}
n <- 7
MSE <- sum(m5$residuals^2)/(n-2)
ave_x <- mean(new_press_data$x_var)
s_2_y_h <- MSE *((1/n) + ((12-ave_x)^2)/sum((new_press_data$x_var - ave_x)^2))
s <- sqrt(s_2_y_h)

yyy <- m5$coef[1] + m5$coef[2]*12

# 99% confidential interval at X12

upper_b <- yyy + qt(0.995,5)*s
under_b <- yyy - qt(0.995,5)*s
upper_b
under_b
```


```{r}


```

